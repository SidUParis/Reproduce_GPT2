{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 滑动平均idea \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a \n",
      " tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "b, \n",
      " tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c,\n",
      " tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n",
      "d \n",
      " tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "print(f\"a \\n {a}\")\n",
    "b = torch.randint(0,10,(3,2)).float() # 0 到10,生成一个size为(3,2)的tensor\n",
    "print(f\"b, \\n {b}\")\n",
    "c = a @ b \n",
    "print(f\"c,\\n {c}\")\n",
    "\n",
    "# 第一行2 是1 0 0 和 2 6 6 ， 7 4 5 的dotproduct \n",
    "# 第二行8 11 是1 1 0 和 2 6 6 , 7 4 5 的dotproduct\n",
    "# 第三航14 16 是1 1 0 和 2 6 6 ， 7 4 5 的dotproduct\n",
    "# 同理，我们可以认为 第一次加了1 个 第二次加了两个，第三次加了3个，那么这个数量和每个row的1的数量相等，我们可以直接做一个normalization\n",
    "# \n",
    "d = c / torch.sum(a,1,keepdim=True)\n",
    "print(F\"d \\n {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time , channel \n",
    "x = torch.randn(B,T,C)\n",
    "# 什么是randn 和rand \n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want x[b,t] = mean_{i<=t} x[b,t] 滑动平均\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # 这个输出只有两个维度，C没有取， 为什么\n",
    "        # previous token are at this batch ndimension, and everything up to and including the t th token, Xprev becomes of shape how many tth element in the path and over C 2dimension\n",
    "        # print(xprev.shape)\n",
    "        xbow[b,t] = torch.mean(xprev,0) # 代表在dimension 0上做平均，最后会得到一个C dimensionl ector\n",
    "        # print(f\"xbow's shape is {xbow[b,t].shape}\") # it becomes 1 dimensional vector of length C,假如说C是2, 那么维度就是[2] \n",
    "        \n",
    "##### xbow的维度为B T C是一个三维，但是当我们说xbow[b,t]的时候，这也就代表了我们在看xbow[b,t,:] 所有的C都考虑，所以当"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解释\n",
    "-0.0894如何来的？ 是(0.1808+(-0.3596))/2  \n",
    "0.1490如何来的？ 是((0.1808+(-0.3596))+0.6258)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "wei's shape torch.Size([8, 8])\n"
     ]
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T,T))\n",
    "print(wei)\n",
    "wei = wei/wei.sum(1,keepdim=True)\n",
    "print(wei) # 0代表col 1代表Row\n",
    "print(f\"wei's shape {wei.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[4, 8, 8]' is invalid for input of size 64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mwei\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[4, 8, 8]' is invalid for input of size 64"
     ]
    }
   ],
   "source": [
    "print(wei.reshape(4,8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xbow2 是这样来的\n",
    "wei的大小是[8 8] 而 x的大小是[4,8,2] 那么在每次Dot product的时候，每一个wei都会和4个[8,2]的矩阵做运算，也就是说会做四次，得到一个[4,8,2] 所以我们也可以理解为 ，wei被“广播” 但实际上不是广播 成了[4,8,8] 然后每次是[i,8,8]和[i,8,2]在做运算， 这样更高效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow2 = wei@x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]],\n",
       "\n",
       "        [[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]],\n",
       "\n",
       "        [[-0.6631, -0.2513],\n",
       "         [ 0.1735, -0.0649],\n",
       "         [ 0.1685,  0.3348],\n",
       "         [-0.1621,  0.1765],\n",
       "         [-0.2312, -0.0436],\n",
       "         [-0.1015, -0.2855],\n",
       "         [-0.2593, -0.1630],\n",
       "         [-0.3015, -0.2293]],\n",
       "\n",
       "        [[ 1.6455, -0.8030],\n",
       "         [ 1.4985, -0.5395],\n",
       "         [ 0.4954,  0.3420],\n",
       "         [ 1.0623, -0.1802],\n",
       "         [ 1.1401, -0.4462],\n",
       "         [ 1.0870, -0.4071],\n",
       "         [ 1.0430, -0.1299],\n",
       "         [ 1.1138, -0.1641]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(xbow,xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 第三种方法 用softmax \n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei3 = torch.zeros((T,T))\n",
    "wei3 = wei3.masked_fill(tril == 0, float('-inf'))\n",
    "print(wei3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "wei3 = F.softmax(wei3,dim=-1)\n",
    "print(wei3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1808, -0.0700],\n",
      "         [-0.0894, -0.4926],\n",
      "         [ 0.1490, -0.3199],\n",
      "         [ 0.3504, -0.2238],\n",
      "         [ 0.3525,  0.0545],\n",
      "         [ 0.0688, -0.0396],\n",
      "         [ 0.0927, -0.0682],\n",
      "         [-0.0341,  0.1332]],\n",
      "\n",
      "        [[ 1.3488, -0.1396],\n",
      "         [ 0.8173,  0.4127],\n",
      "         [-0.1342,  0.4395],\n",
      "         [ 0.2711,  0.4774],\n",
      "         [ 0.2421,  0.0694],\n",
      "         [ 0.0084,  0.0020],\n",
      "         [ 0.0712, -0.1128],\n",
      "         [ 0.2527,  0.2149]],\n",
      "\n",
      "        [[-0.6631, -0.2513],\n",
      "         [ 0.1735, -0.0649],\n",
      "         [ 0.1685,  0.3348],\n",
      "         [-0.1621,  0.1765],\n",
      "         [-0.2312, -0.0436],\n",
      "         [-0.1015, -0.2855],\n",
      "         [-0.2593, -0.1630],\n",
      "         [-0.3015, -0.2293]],\n",
      "\n",
      "        [[ 1.6455, -0.8030],\n",
      "         [ 1.4985, -0.5395],\n",
      "         [ 0.4954,  0.3420],\n",
      "         [ 1.0623, -0.1802],\n",
      "         [ 1.1401, -0.4462],\n",
      "         [ 1.0870, -0.4071],\n",
      "         [ 1.0430, -0.1299],\n",
      "         [ 1.1138, -0.1641]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow3 = wei3@x \n",
    "print(xbow3)\n",
    "torch.allclose(xbow3,xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A case for one head attention \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self attention \n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels \n",
    "x = torch.randn(B,T,C)\n",
    "# single head \n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size,bias=False) # 32,16\n",
    "query = nn.Linear(C,head_size,bias=False)# 32,16\n",
    "\n",
    "k = key(x) # [4,8,32] [32,16] -> [4,8,16]\n",
    "q = query(x) # [4,8,16] B T 16\n",
    "# 在这一步，也就是说一个x会有两个vector k q 到这里这ector没有communication\n",
    "# all the queries will dot product with all the keys \n",
    "# dot_product = q @ k\n",
    "## 但是4 8 16 @ 4 8 16 没办法做点乘，我们需要做transpose, 同时要保证batch \n",
    "wei = q @ k.transpose(-2,-1) # 2个dim -2代表8 -1代表最后一个维度16, 也就是transpose 交换（倒数第二个维度和导读第一个维度） -》 4 16 8 \n",
    "#B T 16 @ B 16 T -> B T T\n",
    "# 所以现在已经不再是一个0 0 comment下面的wei\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "# wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei,dim=-1)\n",
    "\n",
    "out =wei@x \n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后一行的最后一个是第八个token, 他知道他的position和他的context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8077e-01, -6.9988e-02, -3.5962e-01,  ..., -8.0164e-01,\n",
       "           1.5236e+00,  2.5086e+00],\n",
       "         [-5.3029e-01, -2.2274e-01,  7.9457e-01,  ...,  1.1658e+00,\n",
       "           1.5962e+00,  5.5032e-02],\n",
       "         [-5.9427e-01,  3.1861e-01,  5.8950e-02,  ..., -1.8886e-01,\n",
       "          -4.4261e-02,  2.0092e-01],\n",
       "         ...,\n",
       "         [-1.6099e+00,  1.1770e+00,  1.1506e+00,  ..., -4.9985e-02,\n",
       "           1.5179e+00, -5.6034e-01],\n",
       "         [-7.3428e-01,  4.8980e-02,  5.2520e-01,  ...,  8.5586e-01,\n",
       "           1.0967e+00,  3.4021e-01],\n",
       "         [-7.5541e-01, -3.2941e-01,  5.4568e-01,  ..., -9.1224e-02,\n",
       "          -1.4756e-01, -1.3229e-01]],\n",
       "\n",
       "        [[ 4.5618e-01, -1.0917e+00, -8.2073e-01,  ...,  5.1187e-02,\n",
       "          -6.5764e-01, -2.5729e+00],\n",
       "         [ 9.4423e-02,  6.5213e-01, -1.1769e+00,  ...,  6.6192e-01,\n",
       "          -1.0670e+00,  6.5763e-01],\n",
       "         [ 1.6564e+00, -3.7253e-01,  1.1281e-01,  ..., -3.9861e-01,\n",
       "           5.3721e-01, -1.1302e+00],\n",
       "         ...,\n",
       "         [ 3.3062e-01,  8.3752e-01,  7.7502e-01,  ...,  3.6599e-01,\n",
       "          -3.3888e-01,  2.3657e-01],\n",
       "         [ 1.9904e-01,  6.2516e-01, -2.1970e-01,  ...,  2.4208e-01,\n",
       "          -2.8535e-02,  5.8241e-01],\n",
       "         [ 4.4443e-01, -2.8556e-01, -2.1026e-01,  ...,  7.6417e-02,\n",
       "          -7.2575e-02, -7.0739e-01]],\n",
       "\n",
       "        [[-6.0669e-01,  1.8328e+00,  2.9308e-01,  ...,  1.0041e+00,\n",
       "           8.6564e-01,  1.6879e-01],\n",
       "         [-4.1428e-01,  7.4941e-01,  1.4805e-01,  ...,  8.3050e-01,\n",
       "           8.0753e-01, -1.9627e-01],\n",
       "         [-5.9535e-01,  3.9502e-01, -2.4211e-01,  ...,  8.7670e-01,\n",
       "           6.7395e-01, -3.9769e-01],\n",
       "         ...,\n",
       "         [-3.9729e-01, -1.8340e-01,  2.1690e-01,  ..., -1.0661e-01,\n",
       "          -1.8097e-01, -3.1636e-02],\n",
       "         [ 1.6068e-01, -6.7168e-01,  4.0350e-01,  ..., -2.9754e-01,\n",
       "          -3.6188e-01,  1.4013e-01],\n",
       "         [-9.1018e-02,  1.1521e-01,  4.7025e-01,  ...,  2.4925e-01,\n",
       "          -2.1555e-01,  1.7321e-01]],\n",
       "\n",
       "        [[ 3.3299e-01,  1.0995e+00,  4.0335e-01,  ...,  1.6634e+00,\n",
       "          -4.7180e-01,  5.8567e-01],\n",
       "         [-1.3572e-01,  1.0428e+00, -5.4162e-01,  ...,  7.9449e-01,\n",
       "          -2.4049e-01,  2.4993e-01],\n",
       "         [-6.1448e-01,  9.8411e-01, -1.5126e+00,  ..., -1.0812e-01,\n",
       "          -1.0887e-03, -1.0145e-01],\n",
       "         ...,\n",
       "         [-5.7565e-01,  7.6189e-01, -1.5820e+00,  ..., -5.0534e-01,\n",
       "           2.9742e-01, -3.3597e-01],\n",
       "         [ 6.5044e-01,  4.5201e-01,  7.5168e-01,  ...,  1.1093e+00,\n",
       "           1.2900e+00,  9.9372e-01],\n",
       "         [ 2.5062e-01,  2.7916e-01, -3.0415e-01,  ..., -1.3208e-01,\n",
       "           3.4920e-01,  1.9063e-01]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self attention solved this problem :\n",
    "1. every token will emit two vector: query and key \n",
    "2. query : what i m looking for \n",
    "3. key : what do i contain\n",
    "4. we do dot product between query and keys to get the affinity of the token , so my query dot product with all the keys of all the other tokens, it becomes wei here as we defined \n",
    "5. if query and key are aligned, they will interact to a very high amount ,高值代表着更有关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7629, -1.3011,  0.5652,  2.1616, -1.0674,  1.9632,  1.0765, -0.4530],\n",
       "        [-3.3334, -1.6556,  0.1040,  3.3782, -2.1825,  1.0415, -0.0557,  0.2927],\n",
       "        [-1.0226, -1.2606,  0.0762, -0.3813, -0.9843, -1.4303,  0.0749, -0.9547],\n",
       "        [ 0.7836, -0.8014, -0.3368, -0.8496, -0.5602, -1.1701, -1.2927, -1.0260],\n",
       "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,  0.8638,  0.3719,  0.9258],\n",
       "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,  1.4187,  1.2196],\n",
       "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,  0.8048],\n",
       "        [-1.8044, -0.4126, -0.8306,  0.5898, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self attention \n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels \n",
    "x = torch.randn(B,T,C)\n",
    "# single head \n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size,bias=False) # 32,16\n",
    "query = nn.Linear(C,head_size,bias=False)# 32,16\n",
    "\n",
    "k = key(x) # [4,8,32] [32,16] -> [4,8,16]\n",
    "q = query(x) # [4,8,16] B T 16\n",
    "# 在这一步，也就是说一个x会有两个vector k q 到这里这ector没有communication\n",
    "# all the queries will dot product with all the keys \n",
    "# dot_product = q @ k\n",
    "## 但是4 8 16 @ 4 8 16 没办法做点乘，我们需要做transpose, 同时要保证batch \n",
    "wei = q @ k.transpose(-2,-1) # 2个dim -2代表8 -1代表最后一个维度16, 也就是transpose 交换（倒数第二个维度和导读第一个维度） -》 4 16 8 \n",
    "#B T 16 @ B 16 T -> B T T\n",
    "# 所以现在已经不再是一个0 0 comment下面的wei\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "# wei = torch.zeros((T,T))\n",
    "# wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "# wei = F.softmax(wei,dim=-1)\n",
    "\n",
    "out =wei@x \n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一步可以看出，每个query和key的dot product,我们这里没有对WEI做任何的mask处理，也就是说每一个query和每一个key做dot product，即使在第一行代表第一个token\n",
    "但实际上来说，在第一个token，他不应该和后面的token 有什么communication 所以我们用下面的步骤\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7629,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-3.3334, -1.6556,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-1.0226, -1.2606,  0.0762,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.7836, -0.8014, -0.3368, -0.8496,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-1.2566,  0.0187, -0.7880, -1.3204,  2.0363,    -inf,    -inf,    -inf],\n",
       "        [-0.3126,  2.4152, -0.1106, -0.9931,  3.3449, -2.5229,    -inf,    -inf],\n",
       "        [ 1.0876,  1.9652, -0.2621, -0.3158,  0.6091,  1.2616, -0.5484,    -inf],\n",
       "        [-1.8044, -0.4126, -0.8306,  0.5898, -0.7987, -0.5856,  0.6433,  0.6303]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self attention \n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels \n",
    "x = torch.randn(B,T,C)\n",
    "# single head \n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size,bias=False) # 32,16\n",
    "query = nn.Linear(C,head_size,bias=False)# 32,16\n",
    "\n",
    "k = key(x) # [4,8,32] [32,16] -> [4,8,16]\n",
    "q = query(x) # [4,8,16] B T 16\n",
    "# 在这一步，也就是说一个x会有两个vector k q 到这里这ector没有communication\n",
    "# all the queries will dot product with all the keys \n",
    "# dot_product = q @ k\n",
    "## 但是4 8 16 @ 4 8 16 没办法做点乘，我们需要做transpose, 同时要保证batch \n",
    "wei = q @ k.transpose(-2,-1) # 2个dim -2代表8 -1代表最后一个维度16, 也就是transpose 交换（倒数第二个维度和导读第一个维度） -》 4 16 8 \n",
    "#B T 16 @ B 16 T -> B T T\n",
    "# 所以现在已经不再是一个0 0 comment下面的wei\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "# wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "# wei = F.softmax(wei,dim=-1)\n",
    "\n",
    "out =wei@x \n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样 我们就可以通过tril这个triangular 来限制一些communication ，但是有-inf和负数，我们不好做，我们需要一个正常的distribution所以我们exponentiate 并 normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self attention \n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels \n",
    "x = torch.randn(B,T,C)\n",
    "# single head \n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size,bias=False) # 32,16\n",
    "query = nn.Linear(C,head_size,bias=False)# 32,16\n",
    "\n",
    "k = key(x) # [4,8,32] [32,16] -> [4,8,16]\n",
    "q = query(x) # [4,8,16] B T 16\n",
    "# 在这一步，也就是说一个x会有两个vector k q 到这里这ector没有communication\n",
    "# all the queries will dot product with all the keys \n",
    "# dot_product = q @ k\n",
    "## 但是4 8 16 @ 4 8 16 没办法做点乘，我们需要做transpose, 同时要保证batch \n",
    "wei = q @ k.transpose(-2,-1) # 2个dim -2代表8 -1代表最后一个维度16, 也就是transpose 交换（倒数第二个维度和导读第一个维度） -》 4 16 8 \n",
    "#B T 16 @ B 16 T -> B T T\n",
    "# 所以现在已经不再是一个0 0 comment下面的wei\n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "# wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei,dim=-1)\n",
    "\n",
    "out =wei@x \n",
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "但实际上在我们计算attention的时候，我们还有一个V value,sssSsasd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
